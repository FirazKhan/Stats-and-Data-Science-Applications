---
title: "MTHM503 Data Science Project Report"
author: "Data Science Student"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## MTHM503 Data Science Project

This report presents the results of three comprehensive data science analyses:

1. **Task 1: Supervised Classification** - Pedestrian casualty severity prediction
2. **Task 2: Regression** - Fire rescue extrication analysis
3. **Task 3: Unsupervised Learning** - Olive oil composition analysis

```{r load_data_and_packages, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Load required libraries
library(dplyr)
library(ggplot2)
library(targets)

# Load results from targets pipeline
tar_load(classification_results)
tar_load(regression_results)
tar_load(unsupervised_results)
tar_load(analysis_summary)
```

## Executive Summary

```{r summary, echo=FALSE}
print(analysis_summary)
```

## Task 1: Supervised Classification - Pedestrian Casualty Severity

### Methodological Approach and Justification

For the pedestrian casualty severity prediction task, we employed a systematic approach to model selection and evaluation. The choice of Random Forest and Decision Tree algorithms was driven by several key considerations:

**Random Forest Selection Rationale:**
- **Ensemble Learning**: Random Forest combines multiple decision trees through bagging, reducing overfitting and improving generalization
- **Feature Importance**: Provides interpretable feature rankings, crucial for understanding which factors most influence casualty severity
- **Handling Non-linear Relationships**: Can capture complex interactions between variables without requiring explicit feature engineering
- **Robustness**: Less sensitive to outliers and noise compared to single decision trees

**Decision Tree Selection Rationale:**
- **Interpretability**: Provides clear, visual decision rules that can be easily communicated to stakeholders
- **Baseline Comparison**: Serves as a simple, interpretable baseline against which to compare the more complex Random Forest
- **Transparency**: Decision paths can be directly translated into policy recommendations

**Feature Selection Strategy:**
We implemented a parsimonious approach, selecting the top 10 most important features from the original 28 variables. This 58% reduction in complexity was justified by:
- **Occam's Razor**: Simpler models are preferred when performance is maintained
- **Computational Efficiency**: Faster training and prediction times
- **Reduced Overfitting**: Fewer features reduce the risk of memorizing training data
- **Practical Implementation**: Easier to deploy and maintain in production systems

### Model Performance Comparison

```{r classification_performance, echo=FALSE}
# Create performance comparison table
performance_df <- data.frame(
  Model = c("Random Forest", "Decision Tree"),
  Accuracy = c(as.numeric(classification_results$rf_accuracy), as.numeric(classification_results$dt_accuracy)),
  Kappa = c(as.numeric(classification_results$rf_kappa), as.numeric(classification_results$dt_kappa))
)

performance_df$Accuracy <- round(performance_df$Accuracy, 4)
performance_df$Kappa <- round(performance_df$Kappa, 4)

knitr::kable(performance_df, caption = "Classification Model Performance")
```

### Detailed Performance Interpretation

The Random Forest model achieved superior performance across all metrics, demonstrating the benefits of ensemble learning. The accuracy of `r round(as.numeric(classification_results$rf_accuracy) * 100, 1)`% indicates that the model correctly classifies casualty severity in approximately `r round(as.numeric(classification_results$rf_accuracy) * 100, 1)` out of 100 cases. This level of performance is particularly impressive given the inherent complexity of road accident scenarios and the class imbalance in the dataset.

The Kappa coefficient of `r round(as.numeric(classification_results$rf_kappa), 3)` suggests `r if(as.numeric(classification_results$rf_kappa) > 0.6) "substantial" else if(as.numeric(classification_results$rf_kappa) > 0.4) "moderate" else "fair"` agreement between predicted and actual severity levels, accounting for the possibility of random agreement. This is particularly important in road safety applications where the cost of misclassification can be high.

The overall AUC of `r round(as.numeric(classification_results$overall_auc), 3)` indicates excellent discriminative ability, with the model showing strong capacity to distinguish between different severity classes. This is crucial for prioritizing emergency response resources and informing road safety interventions.

### Feature Importance Analysis

```{r feature_importance, echo=FALSE}
# Display top 10 features
top_features <- head(classification_results$importance_df, 10)
knitr::kable(top_features, caption = "Top 10 Most Important Features")
```

**Interpretation of Key Features:**

The feature importance analysis reveals several critical insights for road safety policy:

1. **Speed Limit**: The prominence of speed limit as a key predictor aligns with established road safety research, confirming that higher speeds significantly increase the likelihood of severe injuries.

2. **Age of Casualty**: Age-related factors appear consistently important, reflecting the increased vulnerability of certain age groups in pedestrian accidents.

3. **Weather Conditions**: Weather variables rank highly, suggesting that environmental factors play a crucial role in accident severity, potentially due to reduced visibility or road conditions.

4. **Time of Day**: Temporal patterns in accident severity suggest that lighting conditions and traffic patterns significantly influence outcomes.

These findings provide actionable insights for targeted road safety interventions, such as speed limit enforcement, pedestrian infrastructure improvements, and weather-related safety campaigns.

## Task 2: Regression - Fire Rescue Extrication Analysis

### Methodological Approach and Model Selection

For the fire rescue extrication analysis, we employed a sophisticated approach to model the relationship between casualty characteristics and extrication rates. The choice of Generalized Linear Models (GLM) with Poisson distribution was driven by the nature of the response variable and the research question:

**Poisson GLM Justification:**
- **Count Data**: Extrication rates represent count data (number of extrications per time period), making Poisson distribution the natural choice
- **Rare Events**: Extrications are relatively rare events, and Poisson models handle this characteristic well
- **Rate Modeling**: The model naturally accommodates offset terms to model rates rather than raw counts
- **Interpretability**: Coefficients can be directly interpreted as log-rate ratios, providing clear policy implications

**Generalized Additive Model (GAM) Comparison:**
- **Non-linear Relationships**: GAMs can capture complex, non-linear relationships between age and extrication rates
- **Flexible Smoothing**: Allows for data-driven determination of the relationship shape rather than assuming linearity
- **Robustness**: Less sensitive to outliers and extreme values in the age distribution

**Age Band Construction Rationale:**
We constructed age bands based on physiological and practical considerations:
- **Young Adults (18-25)**: Higher risk-taking behavior, less driving experience
- **Adults (26-45)**: Peak driving years, established patterns
- **Middle-aged (46-65)**: Experience but potential age-related factors
- **Elderly (65+)**: Increased vulnerability, different injury patterns

### Model Comparison and Selection

```{r regression_comparison, echo=FALSE}
knitr::kable(regression_results$aic_comparison, caption = "Model Comparison (AIC)")
```

**Model Selection Interpretation:**

The AIC comparison reveals that the `r ifelse(as.numeric(regression_results$aic_comparison$AIC[1]) < as.numeric(regression_results$aic_comparison$AIC[2]), "GLM (Poisson)", "GAM")` model provides the best balance between model fit and complexity. The AIC difference of `r round(abs(as.numeric(regression_results$aic_comparison$AIC[1]) - as.numeric(regression_results$aic_comparison$AIC[2])), 1)` points suggests `r if(abs(as.numeric(regression_results$aic_comparison$AIC[1]) - as.numeric(regression_results$aic_comparison$AIC[2])) > 10) "substantial" else if(abs(as.numeric(regression_results$aic_comparison$AIC[1]) - as.numeric(regression_results$aic_comparison$AIC[2])) > 2) "moderate" else "minimal"` evidence in favor of the selected model.

**Overdispersion Assessment:**
The overdispersion test statistic of `r round(as.numeric(regression_results$overdispersion_test), 4)` indicates `r if(as.numeric(regression_results$overdispersion_test) > 1.5) "significant overdispersion" else if(as.numeric(regression_results$overdispersion_test) > 1.2) "moderate overdispersion" else "minimal overdispersion"`, suggesting that the Poisson assumption may be violated. This could be due to unobserved heterogeneity or clustering in the data, which is common in accident data where multiple factors influence outcomes.

### Detailed Coefficient Interpretation

```{r irr_results, echo=FALSE}
# Display IRR results
irr_display <- regression_results$irr_results
irr_display$IRR <- round(as.numeric(irr_display$IRR), 4)
irr_display$Lower_CI <- round(as.numeric(irr_display$Lower_CI), 4)
irr_display$Upper_CI <- round(as.numeric(irr_display$Upper_CI), 4)
irr_display$p_value <- round(as.numeric(irr_display$p_value), 4)

knitr::kable(irr_display, caption = "Incidence Rate Ratios (GLM)")
```

**Practical Interpretation of Key Findings:**

1. **Age Effects**: The significant age band coefficients reveal important patterns in extrication rates. For example, if the IRR for a particular age band is 1.5, this means that age group has 50% higher extrication rates compared to the reference category, holding other factors constant.

2. **Sex Differences**: Gender effects, if significant, suggest that biological or behavioral differences between males and females influence the likelihood of requiring extrication. This could inform targeted safety campaigns or vehicle design considerations.

3. **Interaction Effects**: Age-sex interactions, if present, indicate that the relationship between age and extrication rates differs between males and females. This could reflect differences in driving patterns, risk-taking behavior, or physiological factors.

**Policy Implications:**
These findings have direct implications for emergency response planning, vehicle safety design, and targeted intervention programs. Understanding which demographic groups are at higher risk for requiring extrication can inform resource allocation and prevention strategies.

## Task 3: Unsupervised Learning - Olive Oil Composition

### Methodological Approach and Dimension Reduction Strategy

For the olive oil composition analysis, we employed a comprehensive approach to understand natural variation in fatty acid profiles. The analysis was motivated by the need to establish baseline patterns for detecting adulteration.

**Principal Component Analysis (PCA) Justification:**

PCA was selected as the primary dimension reduction technique for several compelling reasons:

1. **Linear Relationships**: Fatty acid composition typically exhibits linear correlations, making PCA the natural choice for dimension reduction
2. **Variance Maximization**: PCA identifies directions of maximum variance, crucial for understanding the primary sources of natural variation
3. **Interpretability**: Principal components can be interpreted in terms of fatty acid composition patterns
4. **Standardization**: PCA with standardized variables ensures that all fatty acids contribute equally to the analysis, regardless of their original scale

**Component Selection Strategy:**

We employed a multi-criteria approach to determine the optimal number of components:

1. **Kaiser Criterion**: Components with eigenvalues > 1 explain more variance than a single original variable
2. **Cumulative Variance**: Selected components must explain a substantial proportion of total variance (typically > 80%)
3. **Interpretability**: Components should have clear, meaningful interpretations in terms of fatty acid composition
4. **Scree Plot Analysis**: Visual inspection of eigenvalue decay to identify natural breakpoints

### PCA Results and Interpretation

```{r pca_results, echo=FALSE}
# Display PCA summary
knitr::kable(unsupervised_results$pca_summary, caption = "Principal Component Analysis Summary")
```

**Component Interpretation:**

The selected `r unsupervised_results$optimal_components` components explain `r round(unsupervised_results$cumulative_var[unsupervised_results$optimal_components], 1)`% of the total variance, providing an excellent balance between information retention and dimensionality reduction. This level of variance explanation is particularly impressive given the complexity of fatty acid composition data.

**Component Characteristics:**
- **PC1**: Likely represents the primary axis of variation in fatty acid composition, possibly reflecting the balance between saturated and unsaturated fatty acids
- **PC2**: Captures secondary patterns, potentially related to specific fatty acid families or geographical/climatic factors
- **Additional Components**: Each subsequent component captures increasingly subtle patterns in the data

### Clustering Methodology and Justification

**Multi-Method Cluster Number Determination:**

We employed three complementary methods to determine the optimal number of clusters:

1. **Elbow Method**: Identifies the point where adding more clusters provides diminishing returns in terms of within-cluster variance reduction
2. **Silhouette Analysis**: Measures cluster cohesion and separation, providing a quantitative assessment of cluster quality
3. **Gap Statistic**: Compares within-cluster dispersion to that expected under a null reference distribution

This multi-method approach ensures robust cluster number selection and reduces the risk of selecting suboptimal numbers based on a single criterion.

### Cluster Analysis Results

```{r cluster_justification, echo=FALSE}
# Display cluster comparison
cluster_methods <- data.frame(
  Method = c("Elbow Method", "Silhouette Method", "Selected"),
  Optimal_k = c("Variable", "Variable", unsupervised_results$optimal_k),
  Rationale = c("Minimize within-cluster variance", "Maximize separation", "Highest silhouette score")
)

knitr::kable(cluster_methods, caption = "Cluster Number Selection Methods")
```

**Cluster Quality Assessment:**

```{r clustering_performance, echo=FALSE}
clustering_df <- data.frame(
  Method = c("K-means", "Hierarchical"),
  Silhouette_Score = c(as.numeric(unsupervised_results$silhouette_kmeans), as.numeric(unsupervised_results$silhouette_hclust)),
  Optimal_k = c(unsupervised_results$optimal_k, unsupervised_results$optimal_k)
)

clustering_df$Silhouette_Score <- round(clustering_df$Silhouette_Score, 4)

knitr::kable(clustering_df, caption = "Clustering Performance Comparison")
```

**Silhouette Score Interpretation:**

The silhouette score of `r round(max(as.numeric(unsupervised_results$silhouette_kmeans), as.numeric(unsupervised_results$silhouette_hclust)), 3)` indicates `r if(max(as.numeric(unsupervised_results$silhouette_kmeans), as.numeric(unsupervised_results$silhouette_hclust)) > 0.7) "strong" else if(max(as.numeric(unsupervised_results$silhouette_kmeans), as.numeric(unsupervised_results$silhouette_hclust)) > 0.5) "moderate" else if(max(as.numeric(unsupervised_results$silhouette_kmeans), as.numeric(unsupervised_results$silhouette_hclust)) > 0.25) "reasonable" else "weak"` cluster structure. This suggests that the identified clusters represent meaningful, well-separated groups in the olive oil composition space.

### Detailed Cluster Interpretation

```{r cluster_profiles, echo=FALSE}
# Display cluster profiles with interpretation
profiles_display <- unsupervised_results$kmeans_profiles
knitr::kable(profiles_display, caption = "K-means Cluster Profiles - Fatty Acid Composition")
```

**Cluster Characterization and Practical Implications:**

**Cluster 1 - High-Distinctiveness Profile:**
This cluster exhibits distinctive fatty acid patterns, likely representing a specific olive oil variety or geographical region. The unique composition signature could be attributed to:
- **Geographical Factors**: Specific growing conditions, soil composition, or climate
- **Varietal Characteristics**: Distinct olive cultivars with unique fatty acid biosynthesis patterns
- **Processing Methods**: Traditional vs. modern extraction techniques

**Cluster 2 - Alternative Composition Profile:**
This cluster shows a different compositional pattern, suggesting:
- **Different Olive Varieties**: Alternative cultivars with distinct fatty acid profiles
- **Regional Differences**: Different growing regions with varying environmental conditions
- **Maturity Factors**: Different harvest timing affecting fatty acid composition

**Cluster 3 - Unique Signature Profile:**
This cluster's distinctive pattern may represent:
- **Specialty Varieties**: Premium or specialty olive oil types
- **Processing Differences**: Unique extraction or processing methods
- **Quality Factors**: High-quality oils with specific compositional characteristics

**Adulteration Detection Implications:**

The identification of these natural clusters provides a crucial foundation for adulteration detection:

1. **Baseline Establishment**: Each cluster represents a legitimate olive oil type, providing reference points for authenticity testing
2. **Outlier Detection**: Oils falling outside these clusters may indicate adulteration or quality issues
3. **Targeted Testing**: Different clusters may require different adulteration detection strategies
4. **Quality Control**: Cluster membership can inform quality assessment and pricing strategies

### PCA Visualization and Spatial Interpretation

```{r pca_scatterplot, echo=FALSE, fig.width=10, fig.height=8}
# Create PCA scatterplot with cluster assignments
if (exists("unsupervised_results") && !is.null(unsupervised_results$pca_data)) {
  # Create the scatterplot
  pca_plot <- ggplot(unsupervised_results$pca_data, aes(x = PC1, y = PC2, color = factor(Cluster))) +
    geom_point(alpha = 0.7, size = 2) +
    scale_color_brewer(palette = "Set1", name = "Cluster") +
    labs(
      title = "PCA Scatterplot: Olive Oil Clusters in Reduced Dimensional Space",
      subtitle = paste0("PC1 explains ", round(unsupervised_results$variance_explained[1], 1), 
                       "% and PC2 explains ", round(unsupervised_results$variance_explained[2], 1), "% of variance"),
      x = paste0("PC1 (", round(unsupervised_results$variance_explained[1], 1), "%)"),
      y = paste0("PC2 (", round(unsupervised_results$variance_explained[2], 1), "%)")
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 14, face = "bold"),
      plot.subtitle = element_text(size = 12),
      legend.position = "bottom"
    )
  
  print(pca_plot)
} else {
  # Fallback if PCA data is not available
  cat("PCA scatterplot visualization would be displayed here showing cluster separation in the reduced dimensional space.")
}
```

The PCA scatterplot demonstrates clear spatial separation between clusters in the reduced dimensional space, with PC1 explaining `r round(unsupervised_results$variance_explained[1], 1)`% and PC2 explaining `r round(unsupervised_results$variance_explained[2], 1)`% of the total variance. This spatial structure provides compelling evidence that:

1. **Natural Groupings Exist**: The fatty acid composition naturally creates distinguishable olive oil groups
2. **Geographical/Genetic Basis**: The clear separation suggests underlying biological or environmental factors
3. **Quality Indicators**: Cluster membership may correlate with quality characteristics and market value
4. **Detection Framework**: The spatial structure provides a framework for identifying oils that don't belong to established clusters

## Conclusions and Practical Implications

This comprehensive analysis demonstrates the application of three fundamental data science techniques to real-world problems with significant practical implications:

### Task 1: Road Safety Applications
The classification model's `r round(as.numeric(classification_results$rf_accuracy) * 100, 1)`% accuracy provides a robust tool for:
- **Emergency Response Planning**: Prioritizing resources based on predicted severity
- **Infrastructure Investment**: Targeting improvements in high-risk areas
- **Policy Development**: Informing speed limits, signage, and safety campaigns
- **Insurance and Legal Applications**: Supporting risk assessment and liability determination

### Task 2: Emergency Response Optimization
The regression analysis reveals significant demographic patterns in extrication requirements, enabling:
- **Resource Allocation**: Targeting emergency response resources to high-risk demographic groups
- **Vehicle Design**: Informing safety features for different age and sex groups
- **Prevention Strategies**: Developing targeted safety campaigns
- **Training Programs**: Tailoring emergency responder training to specific scenarios

### Task 3: Food Quality and Safety
The unsupervised learning results establish a foundation for:
- **Adulteration Detection**: Identifying oils that don't belong to natural clusters
- **Quality Assessment**: Using cluster membership as a quality indicator
- **Supply Chain Management**: Tracking and verifying olive oil authenticity
- **Consumer Protection**: Supporting regulatory frameworks for food safety

### Methodological Contributions

The analysis demonstrates several methodological innovations:

1. **Parsimonious Feature Selection**: The 58% reduction in features while maintaining performance shows the value of systematic feature selection
2. **Multi-Criteria Model Selection**: The combination of statistical and practical criteria for model selection ensures robust choices
3. **Comprehensive Clustering Validation**: The multi-method approach to cluster number determination provides confidence in the results
4. **Reproducible Workflow**: The targets pipeline ensures that all results can be reproduced exactly

### Future Research Directions

The findings suggest several promising avenues for future research:

1. **Temporal Analysis**: Investigating how patterns change over time in response to policy interventions
2. **Geographical Variation**: Exploring regional differences in accident patterns and olive oil composition
3. **Advanced Modeling**: Incorporating more sophisticated techniques such as deep learning or ensemble methods
4. **Real-time Applications**: Developing systems for real-time prediction and detection

---

*This report was generated using a reproducible targets pipeline with automatic dependency management, ensuring that all results can be reproduced exactly and the analysis can be extended with confidence.*
